{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial Mixture Model with Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "\n",
    "## Generating Data\n",
    "\n",
    "We first generate some data points which are randomly drawn from a Binomial Mixture Model with two Binomial Distributions. Given $N_i$, the probability of $n_i$ is\n",
    "\n",
    "$P(n_i | N_i, \\Theta) = \\sum_{k=1}^{2}\\pi_k \\mathrm{Bino}(n_i|N_i, \\theta_k)$, \n",
    "\n",
    "where the Binomial Distribution is\n",
    "\n",
    "$\\mathrm{Bino}(n_i|N_i, \\theta) = {N_i!\\over n_i!(N_i-n_i)!} \\theta^{n_i} (1-\\theta)^{N_i-n_i}$,\n",
    "\n",
    "and the sum of $\\pi$'s is unity, i.e.\n",
    "\n",
    "$\\sum_{k=1}^{2} \\pi_k = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is unavailable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.binomial import Binomial\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    import torch.cuda as t\n",
    "else:\n",
    "    print(\"cuda is unavailable\")\n",
    "    import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = int(1e6)\n",
    "\n",
    "# kmer length\n",
    "k_len = 31\n",
    "\n",
    "# the theta's the two Binomial Distributions\n",
    "theta_err = 0.999\n",
    "theta_cor = 1-theta_err\n",
    "\n",
    "# the probabilities, pi's, of the two Binomial Distributions\n",
    "pi_err = 0.0001\n",
    "pi_cor = 1.0 - pi_err\n",
    "\n",
    "# the list of (Ni| i =1, 2, ..., S), uniformly drawn between low and high\n",
    "N_ls_all = t.full((S,), k_len)\n",
    "N_ls_all = N_ls_all.type(t.FloatTensor)\n",
    "\n",
    "# the list of theta, each theta is either theta_1 or theta_2. The probability of theta_i is pi_i\n",
    "theta_ls = t.FloatTensor(np.random.choice([theta_err,theta_cor], size=S, p=[pi_err,pi_cor]))\n",
    "\n",
    "# the list of (ni | i=1,2 ...,S)\n",
    "n_ls_all = Binomial(N_ls_all, theta_ls).sample()\n",
    "\n",
    "#errors\n",
    "ground_truth = (theta_ls > theta_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some figures to get some visual impression of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " import matplotlib.pyplot as plt\n",
    "# %matplotlib inline \n",
    "\n",
    "# fig, axes = plt.subplots(1,2,figsize=(14,6))\n",
    "# axes[0].scatter(N_ls_all-n_ls_all, n_ls_all)\n",
    "# axes[0].set_xlabel(\"N-n\",size=16)\n",
    "# axes[0].set_ylabel(\"n\",size=16)\n",
    "# axes[0].tick_params(labelsize=14)\n",
    "\n",
    "# axes[1].hist(n_ls_all/N_ls_all, bins=20)\n",
    "# axes[1].set_xlabel(\"n/N\", size=16)\n",
    "# axes[1].tick_params(labelsize=14)\n",
    "# axes[1].set_title(\"Histogram of n/N\", size=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into train and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "S = len(N_ls_all)\n",
    "shuffled_indice = torch.randperm(S)\n",
    "N_ls_shuffled = N_ls_all[shuffled_indice]\n",
    "n_ls_shuffled = n_ls_all[shuffled_indice]\n",
    "\n",
    "# percentage of train set.\n",
    "train_frac = 0.7\n",
    "train_index = int(0.7*S)\n",
    "N_ls_train = N_ls_shuffled[0:train_index]\n",
    "N_ls_valid = N_ls_shuffled[train_index:]\n",
    "n_ls_train = n_ls_shuffled[0:train_index]\n",
    "n_ls_valid = n_ls_shuffled[train_index:]\n",
    "\n",
    "#reorder ground truth\n",
    "ground_truth = ground_truth[shuffled_indice]\n",
    "ground_truth_train = ground_truth[0:train_index]\n",
    "ground_truth_valid = ground_truth[train_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the `log_likelihood` \n",
    "\n",
    "The `log_likelihood` is the log of the probability of the parameters, $\\Theta$, given the observed data, `N_ls` and `n_ls`. It is defined below,\n",
    "\n",
    "`log_likelihood` $= \\ln(L(\\Theta, {ni})) =\\ln( P({ni} | \\Theta)) = \\sum_{i=1}^{S} \\ln(\\sum_{k=1}^{K} \\pi_k * \\mathrm{Binom}(n_i|N_i, \\theta_k) )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate log_likelihood using a method which supposes to avoid underflow or overflow in\n",
    "# log_sum_exp = log(sum_{i=1}^{S} exp(bi)). When bi >> 1, overflow leads to log_sum_exp = infty\n",
    "# When bi << -1, underflow leads to log_sum_exp = -infty.\n",
    "\n",
    "def calc_logL(N_ls, n_ls, pi_list, theta_list):\n",
    "    '''\n",
    "    Input: N_ls is a [S] shape tensor = [N1, N2, ..., NS]\n",
    "           n_ls is a [S] shape tensor = [n1, n2, ..., nS]\n",
    "           pi_list is a [K] shape tensor = [pi_1, .., pi_K]\n",
    "           theta_list is a [K] shape tensor = [theta_1, ..., theta_K]\n",
    "    Output: log_likelihood of the parameters (pi and theta) given the observed data (N, n).\n",
    "    '''\n",
    "    S = len(N_ls)\n",
    "    K = len(pi_list)\n",
    "\n",
    "    # log_binom_mat has shape (S,K), element_{i,l} = log_Binomial(ni|Ni, theta_l)\n",
    "    # log with natural base.\n",
    "    log_binom_mat = Binomial(N_ls.reshape(S,1), theta_list.reshape(1,K)).log_prob(n_ls.reshape(S,1))\n",
    "\n",
    "    # mean_log_binom, the mean value of all elements in log_binom_mat.\n",
    "    c = torch.mean(log_binom_mat)\n",
    "\n",
    "    # binom_mat has shape (S,K), element_{i,l} = Binomial(ni|Ni, theta_l)\n",
    "    binom_mat = torch.exp(log_binom_mat - c)\n",
    "\n",
    "    # log_likelihood = sum_{i=1}^{S} log(prob_i), this is a real number\n",
    "    log_likelihood = S*c + torch.sum(torch.log(torch.matmul(binom_mat, pi_list)))\n",
    "\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $P(z_i=m| n_i, \\Theta_\\mathrm{old})$\n",
    "\n",
    "$P(z_i=m| n_i, \\Theta_\\mathrm{old}) = \\left[\\sum_{l=1}^{K} {\\pi_{l,old}\\over \\pi_{m,old}}\\left(\\theta_{l,old}\\over\\theta_{m,old}\\right)^{n_i}\\left({1-\\theta_{l,old}}\\over{1-\\theta_{m,old}}\\right)^{N_i-n_i}\\right]^{-1}$\n",
    "\n",
    "We take advantage of the [broadcast](https://pytorch.org/docs/stable/notes/broadcasting.html) nature for torch tensors. Any torch math manipulations, except element-wise manipulations, would take action in a broadcast way as long as the tensors are broadcastable. Broadcasting does not make copy of the data on the memory and thus is very efficient and much more preferred to `for loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Posterior(N_ls, n_ls, pi_list, theta_list):\n",
    "    '''\n",
    "    Input: N_ls is a [S] shape tensor = [N1, N2, ..., NS]\n",
    "           n_ls is a [S] shape tensor = [n1, n2, ..., nS]\n",
    "           pi_list is a [K] shape tensor = [pi_1, .., pi_K]\n",
    "           theta_list is a [K] shape tensor = [theta_1, ..., theta_K]\n",
    "    Output: Posterior, a tensor with shape (K,S) and its element_{m,i} = P(zi=m|ni,Theta_old) which is\n",
    "            the posterior probability of the i-th sample belonging to the m-th Binomial distribution.\n",
    "    '''\n",
    "    \n",
    "    # shape = (K,K) with theta_ratio_{m,l} = theta_l/theta_m, m-th row, l-th column\n",
    "    theta_ratio = torch.div(theta_list.reshape(1,K), theta_list.reshape(K,1))\n",
    "\n",
    "    # shape = (K,K), element_{ml} = (1-theta_l)/(1-theta_m)\n",
    "    unity_minus_theta_ratio = torch.div((1e0 - theta_list).reshape(1,K), (1e0 - theta_list).reshape(K,1))\n",
    "\n",
    "    # shape = (K,K), element_{m,l} = (theta_l/theta_m) * [(1-theta_l)/(1-theta_m)]\n",
    "    mixed_ratio = torch.mul(theta_ratio, unity_minus_theta_ratio)\n",
    "\n",
    "    # shape = (K,K,S) with element_{m,l,i} = [(theta_l/theta_m)*(1-theta_l)/(1-theta_m)]^ni\n",
    "    # its element won't be either 0 or infty no matther whether theta_l > or < theta_m\n",
    "    mixed_ratio_pow = torch.pow(theta_ratio.reshape(K,K,1), n_ls)\n",
    "    mixed_ratio_pow = torch.clamp(mixed_ratio_pow, min=0.0, max=1e15)\n",
    "\n",
    "    # shape = (K,K,S) with element_{m,l,i} = [ (1-theta_l)/(1-theta_m) ]^(Ni-2ni)\n",
    "    # its element may be infty if theta_l<<theta_m, or 0 if theta_l >> theta_m\n",
    "    unity_minus_theta_ratio_pow = torch.pow(unity_minus_theta_ratio.reshape(K,K,1), N_ls-2.0*n_ls)\n",
    "    unity_minus_theta_ratio_pow = torch.clamp(unity_minus_theta_ratio_pow, min=0.0, max=1e15)\n",
    "\n",
    "    # In below, we multiply the element of mixed_ratio_pow and the element of unity_minus_theta_ratio_pow,\n",
    "    # and there won't be nan caused by 0*infty or infty*0 because the element in mixed_ratio_pow won't be 0 or infty.\n",
    "    # Thus we make sure there won't be nan in Posterior.\n",
    "\n",
    "    # element-wise multiply, pow_tensor has shape(K,K,S), element_{m,l,i} = (theta_l/theta_m)^ni * [(1-theta_l)/(1-theta_m)]^(Ni-ni).\n",
    "    # Note that torch.mul(a, b) would broadcast if a and b are different in shape & they are\n",
    "    # broadcastable. If a and b are the same in shape, torch.mul(a,b) would operate element-wise multiplication.\n",
    "\n",
    "    pow_tensor = torch.mul(mixed_ratio_pow, unity_minus_theta_ratio_pow)\n",
    "\n",
    "    # pi_ratio has shape (K,K) with element_{m,l} = pi_l/pi_m\n",
    "    pi_ratio = torch.div(pi_list.reshape(1,K), pi_list.reshape(K,1))\n",
    "\n",
    "    # posterior probability tensor, Pzim = P(zi=m|ni,Theta_old)\n",
    "    # shape (K,S), element_{m,i} = P(zi=m|ni,Theta_old)\n",
    "    S = len(N_ls)\n",
    "    Posterior = torch.pow(torch.matmul(pi_ratio.reshape(K,1,K), pow_tensor), -1e0).reshape(K,S)\n",
    "\n",
    "    return Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update $\\Theta \\equiv \\{(\\pi_m, \\theta_m)|m=1,2,...,K\\}$ According to the EM Algorithm\n",
    "\n",
    "The computational complexity of the EM Algorithm is $S\\times K$ per iteration.\n",
    "\n",
    "$\\pi_m ={1\\over S} \\sum_{i=1}^{S} P(z_i=m| n_i, \\Theta_{old})$\n",
    "\n",
    "$\\theta_m = {{\\sum_{i=1}^{S} n_i P(z_i=m| n_i, \\Theta_{old})}\\over{\\sum_{j=1}^{S} N_j P(z_j=m| n_j, \\Theta_{old})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(N_ls, n_ls, Posterior):\n",
    "    '''\n",
    "    Input: N_ls, tensor of shape [S]\n",
    "           n_ls, tensor of shape [S]\n",
    "           Posterior, tensor of shape (K,S)\n",
    "    '''\n",
    "    # update pi_list\n",
    "    # torch.sum(tensor, n) sum over the n-th dimension of the tensor\n",
    "    # e.g. if tensor'shape is (K,S) and n=1, the resulting tensor has shape (K,)\n",
    "    # the m-th element is the sum_{i=1}^{S} tensor_{m,i}\n",
    "    pi_list = torch.sum(Posterior,1)/len(N_ls)\n",
    "\n",
    "    # update theta_list\n",
    "    theta_list = torch.div(torch.matmul(Posterior, n_ls), torch.matmul(Posterior, N_ls))\n",
    "\n",
    "    return pi_list, theta_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to train on the training set. So we make the following assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ls = N_ls_train\n",
    "n_ls = n_ls_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Data by a Binomial Mixture Model\n",
    "\n",
    "The method would fit the parameters\n",
    "\n",
    "$\\Theta = \\{ (\\pi_k, \\theta_k) | k=1, 2, ..., K\\}$\n",
    "\n",
    "We need to pre-set K. Here we set $K=2$. Of course, in reality we would not know the best $K$ to adopt. We will discuss how to choose $K$ after this section.\n",
    "\n",
    "\n",
    "## Step 1: Initializing the parameters $\\Theta$\n",
    "\n",
    "We denote $\\Theta$ by `params` in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set K\n",
    "K = 2\n",
    "# choose a very small positive real number \n",
    "small_value = 1e-6\n",
    "\n",
    "# initialize pi's, make sure that the sum of all pi's is unity\n",
    "# pi is drawn from a Uniform distribution bound by [small_value, 1)\n",
    "from torch.distributions.uniform import Uniform\n",
    "pi_list = Uniform(low=small_value, high=1e0).sample([K-1])\n",
    "pi_K = t.FloatTensor([1e0]) - pi_list.sum()\n",
    "pi_list = torch.cat([pi_list, pi_K], dim=0)\n",
    "\n",
    "# initialize theta's, make sure that each theta satisfies 0<theta<1\n",
    "from torch.distributions.normal import Normal\n",
    "theta_list = torch.clamp(Normal(loc=0.5, scale=0.3).sample(t.IntTensor([K])), min=small_value, max=1e0-small_value)\n",
    "\n",
    "# combine all pi and theta into a list of tuples called `params`, which is the capital Theta in my article\n",
    "# params has the shape of K rows x 2 columns\n",
    "params = torch.stack([pi_list, theta_list], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setting Up Conditions for Stopping the Iteration\n",
    "\n",
    "* Calculate the `log_likelihood`.\n",
    "    \n",
    "    \n",
    "* Initialize the change of the `log_likelihood` named `delta_log_likelihood` and the iteration step `iter_step`.\n",
    "\n",
    "\n",
    "* Set the lower bound for `delta_log_likelihood` named `tolerance`  and the upper bound for the `iter_step` named `max_step`.\n",
    "\n",
    "\n",
    "* Define conditions for the iteration to continue. If either condition fails, the iteration stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the initial log_likelihood\n",
    "log_likelihood = calc_logL(N_ls, n_ls, pi_list, theta_list)\n",
    "\n",
    "# initialize the change of log_likelihood named `delta_log_likelihood` and the iteration step called `iter_step`\n",
    "delta_log_likelihood = torch.norm(log_likelihood)\n",
    "iter_step = 0\n",
    "\n",
    "# tolerance for the change of the log-likelihood\n",
    "tolerance = 1e-6\n",
    "\n",
    "# set the maximum steps for iteration, stop the iteration if the number of steps reaches `max_step` \n",
    "max_step = int(1e2)\n",
    "\n",
    "# The iteration stops when either of the following two conditions is broken first\n",
    "cond_likelihood = (delta_log_likelihood > tolerance)\n",
    "cond_step = t.BoolTensor([iter_step < max_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Iteration using the EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2:\n",
      "delta_log_likelihood = nan\n",
      "log_likelihood =inf\n",
      "tensor([[1.0286e-04, 9.9821e-01],\n",
      "        [9.9990e-01, 9.9181e-04]])\n",
      "used 0.2490239143371582\n",
      "P=72\n",
      "N=699928\n",
      "TP=72\n",
      "TN=699928\n",
      "FP=0\n",
      "FN=0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "cond_step = True\n",
    "cond_likelihood = True\n",
    "iter_step = 0\n",
    "\n",
    "while cond_step and cond_likelihood:\n",
    "\n",
    "    # posterior probability tensor, Pzim = P(zi=m|ni,Theta_old)\n",
    "    # shape (K,S), element_{m,i} = P(zi=m|ni,Theta_old)\n",
    "    Posterior = calc_Posterior(N_ls, n_ls, pi_list, theta_list)\n",
    "    \n",
    "    # calculate the new pi_list and theta_list\n",
    "    pi_list_new, theta_list_new = calc_params(N_ls, n_ls, Posterior)\n",
    "\n",
    "    # calculate the new log_likelihood\n",
    "    log_likelihood_new = calc_logL(N_ls, n_ls, pi_list_new, theta_list_new)\n",
    "    \n",
    "    # calculate the change of the log-likelihood\n",
    "    delta_log_likelihood = torch.norm(log_likelihood_new - log_likelihood)\n",
    "    \n",
    "    # update params \n",
    "    pi_list = pi_list_new\n",
    "    theta_list = theta_list_new\n",
    "    \n",
    "    # update log_likelihood\n",
    "    log_likelihood = log_likelihood_new\n",
    "    \n",
    "    # increase iter_step by 1\n",
    "    iter_step += 1\n",
    "\n",
    "    # update the conditions for the while loop\n",
    "    # cond_params = (delta_params > epsilon)\n",
    "    cond_likelihood = (delta_log_likelihood > tolerance)\n",
    "    cond_step = t.BoolTensor([iter_step < max_step])\n",
    "    \n",
    "print(f\"Iteration {iter_step}:\")\n",
    "print(f\"delta_log_likelihood = {delta_log_likelihood}\")\n",
    "print(f\"log_likelihood ={log_likelihood}\")\n",
    "params = torch.stack([pi_list, theta_list], dim=1)\n",
    "print(f\"{params}\")\n",
    "    \n",
    "print(f\"used {time.time()-start_time}\")\n",
    "\n",
    "if Posterior[0].mean() > 0.5:\n",
    "    Posterior_assignments = Posterior[0] < 0.5\n",
    "else:\n",
    "    Posterior_assignments = Posterior[0] > 0.5\n",
    "\n",
    "P = (ground_truth_train == True).sum()\n",
    "N = (ground_truth_train == False).sum()\n",
    "\n",
    "print(\"P={0}\\nN={1}\".format(P, N))\n",
    "\n",
    "TP = ((Posterior_assignments == ground_truth_train) & (Posterior_assignments == True)).sum()\n",
    "TN = ((Posterior_assignments == ground_truth_train) & (Posterior_assignments == False)).sum()\n",
    "FP = ((Posterior_assignments != ground_truth_train) & (Posterior_assignments == True)).sum()\n",
    "FN = ((Posterior_assignments != ground_truth_train) & (Posterior_assignments == False)).sum()\n",
    "\n",
    "print(\"TP={0}\\nTN={1}\\nFP={2}\\nFN={3}\".format(TP, TN, FP, FN))\n",
    "\n",
    "#plt.hist(Posterior[0], bins='auto')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the number of components, K\n",
    "\n",
    "The EM Algorithm above fits a BMM model to the data with a preset K, the number of components. However, in reality, we usually do not know K. Therefore, we need to find an optimal K.\n",
    "\n",
    "We look at three metrics. \n",
    "\n",
    "* `log_likelihood`: $\\log L$, the probability of the parameters, $\\Theta$, given the observed data set, {ni, Ni|i=1,..,S}.\n",
    "\n",
    "\n",
    "* Akaike Information Criterion (AIC): AIC $= -{2\\over S}\\log L + {2 (2K+1)\\over S}$, where $(2K+1)$ is the number of parameters in a BMM model.\n",
    "\n",
    "\n",
    "* Bayesian Information Criterion (BIC): BIC $= -2\\log L + (2K+1)\\log S$.\n",
    "\n",
    "The more complicated the model is, the more parameters, and thus greater $(2K+1)$. A more complicated model generally fits the data better and thus results in greater `log_likelihood`, $\\log L$. However, a complicated model may overfit the data, meaning that it fits the training data set well but generalizes poorly to new data. Therefore, one should put some penalty on the complexity of the model, which is taken care of by both AIC and BIC. \n",
    "\n",
    "The following cell fits a number of BMM models on the training data sets with various values for $K$. The the three metrics above are calculated and stored in `logL_list`, `AIC_list`, `BIC_list`. Then the trained BMM models are applied to the validation set and calculate the corresponding metrics. The metrics are stored in `logL_val_list`, `AIC_val_list`, `BIC_val_list`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q2/ysdf78ns749_sx0cj8l8d5040000gn/T/ipykernel_70656/2060754693.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# list of n/N sorted in ascending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mratio_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_array\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_array' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------- Initialization -------------------------------------\n",
    "# Sample size\n",
    "S = len(N_ls)\n",
    "S_val = len(N_ls_valid)\n",
    "\n",
    "K_list = range(2, 9)\n",
    "params_list = []\n",
    "\n",
    "logL_list = []\n",
    "AIC_list = []\n",
    "BIC_list = []\n",
    "\n",
    "logL_val_list = []\n",
    "AIC_val_list = []\n",
    "BIC_val_list = []\n",
    "\n",
    "# Set K, the number of Binomial distributions in the to-be-fitted mixture model\n",
    "for K in K_list:\n",
    "\n",
    "    '''\n",
    "    Initialize theta_list and pi_list\n",
    "    '''\n",
    "\n",
    "    # list of n/N sorted in ascending order\n",
    "    ratio_ls = np.sort(n_array/N_array)\n",
    "\n",
    "    # reproducibility\n",
    "    np.random.seed(seed=123)\n",
    "\n",
    "    # pick K random integer indice, [index_1, ..., index_K]\n",
    "    random_indice = np.sort(np.random.choice(len(ratio_ls),K))\n",
    "    # theta are the ratio at the random indice, { ratio_ls[index_k] | k=1,2,...,K }\n",
    "    theta_array = ratio_ls[random_indice]\n",
    "\n",
    "\n",
    "    # the proportion of the midpoint of each pair of consecutive indice\n",
    "    # (index_k + index_{k+1})/2/len(ratio_ls), for k=1,2,...,K-1\n",
    "    acc_portion_ls = (random_indice[1:]+random_indice[:-1])/2.0/len(ratio_ls)\n",
    "    acc_portion_ls = np.append(acc_portion_ls, 1.0)\n",
    "    # initialize pi_list using the portions of indice\n",
    "    pi_array = np.insert(acc_portion_ls[1:] - acc_portion_ls[:-1], obj=0, values=acc_portion_ls[0])\n",
    "\n",
    "    print(f\"initial theta's are {theta_array}\")\n",
    "    print(f\"initial pi's are {pi_array}\")\n",
    "    print(f\"sum of all pi's is {pi_array.sum()}.\")\n",
    "\n",
    "    # convert numpy arrays to torch tensors.\n",
    "    theta_list = t.FloatTensor(theta_array)\n",
    "    pi_list = t.FloatTensor(pi_array)\n",
    "    print(f\"theta_list is on device {theta_list.get_device()}\")\n",
    "    print(f\"pi_list is on device {pi_list.get_device()}\")\n",
    "\n",
    "    # combine all pi and theta into a list of tuples called `params`, which is referred to by Theta as well\n",
    "    # params has the shape of K rows x 2 columns\n",
    "    params = torch.stack([pi_list, theta_list], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------Setting stop conditions for the EM-Algorithm iterations---------------\n",
    "    '''\n",
    "    Conditions for the EM-Algorithm Iteration to Continue.\n",
    "    '''\n",
    "\n",
    "    # calculate the initial log_likelihood\n",
    "    log_likelihood = calc_logL(N_ls, n_ls, pi_list, theta_list)\n",
    "    log_likelihood_init = log_likelihood\n",
    "    print(f\"Initial log_likelihood is {log_likelihood}\")\n",
    "\n",
    "    # initialize the change of log_likelihood named `delta_log_likelihood` and the iteration step called `iter_step`\n",
    "    delta_log_likelihood = torch.abs(log_likelihood)\n",
    "    iter_step = 0\n",
    "    delta_params = torch.norm(params.reshape(2*K,))\n",
    "\n",
    "    # tolerance for the change of the log-likelihood or the change of params, depending on\n",
    "    # which condition you decide to use\n",
    "    logL_eps = 1e-2\n",
    "    param_eps = 1e-5\n",
    "    # set the maximum steps for iteration, stop the iteration if the number of steps reaches `max_step`\n",
    "    max_step = int(1e4)\n",
    "\n",
    "    # we define 3 conditions:\n",
    "    # the condition below is that \"the log_likelihood are still changing much\"\n",
    "    if torch.isnan(delta_log_likelihood) :\n",
    "        cond_likelihood = True\n",
    "    else:\n",
    "        cond_likelihood = (torch.abs(delta_log_likelihood) > logL_eps)\n",
    "    # the condition below is that \"the iteration steps have not exceeded max_step\"\n",
    "    cond_step = t.BoolTensor([iter_step < max_step])\n",
    "    # the condition below is that \"the params are still changing much\"\n",
    "    cond_params = (delta_params >param_eps)\n",
    "\n",
    "\n",
    "\n",
    "    # -------------- Iteration Loop -------------------------------------------------------\n",
    "    '''\n",
    "    EM-Algorithm Iterations\n",
    "    '''\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # the second condition below may be either `cond_likelihood` or `cond_params`\n",
    "    while cond_step & cond_params:\n",
    "\n",
    "        # posterior probability tensor, Pzim = P(zi=m|ni,Theta_old)\n",
    "        # shape (K,S), element_{m,i} = P(zi=m|ni,Theta_old)\n",
    "        Posterior = calc_Posterior(N_ls, n_ls, pi_list, theta_list)\n",
    "\n",
    "        # print(f\"Does Posterior contain nan? {torch.isnan(Posterior).any()}\")\n",
    "\n",
    "        # calculate the new pi_list and theta_list\n",
    "        pi_list_new, theta_list_new =  calc_params(N_ls, n_ls, Posterior)\n",
    "        params_new = torch.stack([pi_list_new, theta_list_new], dim=1)\n",
    "\n",
    "        # calculate the new log_likelihood\n",
    "        log_likelihood_new = calc_logL(N_ls, n_ls, pi_list_new, theta_list_new)\n",
    "\n",
    "        # calculate the change of the log-likelihood\n",
    "        delta_log_likelihood = log_likelihood_new - log_likelihood\n",
    "\n",
    "        # calculate the change of params\n",
    "        delta_params = torch.norm(params_new.reshape(2*K,)-params.reshape(2*K,))\n",
    "\n",
    "        # update params\n",
    "        pi_list = pi_list_new\n",
    "        theta_list = theta_list_new\n",
    "        params = params_new\n",
    "\n",
    "        # update log_likelihood\n",
    "        log_likelihood = log_likelihood_new\n",
    "\n",
    "        # increase iter_step by 1\n",
    "        iter_step += 1\n",
    "\n",
    "        # update the conditions for the while loop\n",
    "\n",
    "        if torch.isnan(delta_log_likelihood) :\n",
    "            cond_likelihood = True\n",
    "        else:\n",
    "            cond_likelihood = (torch.abs(delta_log_likelihood) > logL_eps)\n",
    "\n",
    "        cond_step = t.BoolTensor([iter_step < max_step])\n",
    "        cond_params = (delta_params > param_eps)\n",
    "\n",
    "        # if iter_step % 5 == 0:\n",
    "        #     print(f\"Iteration {iter_step}:\")\n",
    "        #     print(f\"logL = {log_likelihood:.6f}\")\n",
    "        #     # print(f\"logL - logL_init = {log_likelihood - log_likelihood_init}\")\n",
    "        #     print(f\"delta_logL = {delta_log_likelihood:.6f}\")\n",
    "        #     print(f\"delta_params = {delta_params:.6f}\")\n",
    "        #     print(f\"{params}\")\n",
    "\n",
    "    # calculate Akaike Information Criterion (AIC)\n",
    "    AIC = -2.0/float(S)*log_likelihood + 2.0*(2.0*float(K)+1.0)/float(S)\n",
    "\n",
    "    # Bayesian Information Criterion\n",
    "    BIC = -2.0*log_likelihood + np.log(float(S))*(2.0*float(K)+1.0)\n",
    "\n",
    "    #  calculate metrics for the validation sets\n",
    "    log_likelihood_val = calc_logL(N_ls_valid, n_ls_valid, pi_list, theta_list)\n",
    "    AIC_val = -2.0/float(S_val)*log_likelihood_val + 2.0*(2.0*float(K)+1.0)/float(S_val)\n",
    "    BIC_val = -2.0*log_likelihood_val + np.log(float(S_val))*(2.0*float(K)+1.0)\n",
    "\n",
    "    print(f\"used {time.time()-start_time} seconds.\")\n",
    "    print(\"Final Results:\")\n",
    "    print(f\"Iteration {iter_step}:\")\n",
    "    print(f\"logL = {log_likelihood:.6f}\")\n",
    "    # print(f\"logL - logL_init = {log_likelihood - log_likelihood_init}\")\n",
    "    print(f\"delta_log_likelihood = {delta_log_likelihood:.6f}\")\n",
    "    print(f\"delta_params = {delta_params:.6f}\")\n",
    "    params = torch.stack([pi_list, theta_list], dim=1)\n",
    "    print(f\"{params}\")\n",
    "    print(f\"log_binom_min, max, mean = {log_binom_minmaxmean(N_ls, n_ls, pi_list, theta_list)}\")\n",
    "    print(f\"Akaike Information Criterion (AIC) = {AIC:.6f}\")\n",
    "    print(f\"Bayesian Information Criterion (BIC) = {BIC:.6f}\")\n",
    "\n",
    "    logL_list.append(log_likelihood)\n",
    "    AIC_list.append(AIC)\n",
    "    BIC_list.append(BIC)\n",
    "    params_list.append(params)\n",
    "\n",
    "    logL_val_list.append(log_likelihood_val)\n",
    "    AIC_val_list.append(AIC_val)\n",
    "    BIC_val_list.append(BIC_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
