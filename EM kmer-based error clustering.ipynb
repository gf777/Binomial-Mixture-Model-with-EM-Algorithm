{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial Mixture Model with Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "## The Binomial distribution\n",
    "\n",
    "The Given $N_i$, the probability of $n_i$ is\n",
    "\n",
    "$P(n_i | N_i, \\Theta) = \\sum_{k=1}^{2}\\pi_k \\mathrm{Bino}(n_i|N_i, \\theta_k)$, \n",
    "\n",
    "where the Binomial Distribution is\n",
    "\n",
    "$\\mathrm{Bino}(n_i|N_i, \\theta) = {N_i!\\over n_i!(N_i-n_i)!} \\theta^{n_i} (1-\\theta)^{N_i-n_i}$,\n",
    "\n",
    "and the sum of $\\pi$'s is unity, i.e.\n",
    "\n",
    "$\\sum_{k=1}^{2} \\pi_k = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is unavailable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.binomial import Binomial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    import torch.cuda as t\n",
    "else:\n",
    "    print(\"cuda is unavailable\")\n",
    "    import torch as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataset(S, k_len, theta_err, theta_cor, pi_err):\n",
    "\n",
    "    # probability of a base being correct\n",
    "    pi_cor = 1.0 - pi_err\n",
    "\n",
    "    # the list of (Ni| i =1, 2, ..., S), uniformly drawn between low and high\n",
    "    N_ls_all = t.full((S,), k_len)\n",
    "    N_ls_all = N_ls_all.type(t.FloatTensor)\n",
    "\n",
    "    # the list of theta, each theta is either theta_1 or theta_2. The probability of theta_i is pi_i\n",
    "    theta_ls = t.FloatTensor(np.random.choice([theta_err,theta_cor], size=S, p=[pi_err,pi_cor]))\n",
    "\n",
    "    # the list of (ni | i=1,2 ...,S)\n",
    "    n_ls_all = Binomial(N_ls_all, theta_ls).sample()\n",
    "\n",
    "    #errors\n",
    "    ground_truth = (theta_ls > theta_cor)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    shuffled_indice = torch.randperm(S)\n",
    "    N_ls_shuffled = N_ls_all[shuffled_indice]\n",
    "    n_ls_shuffled = n_ls_all[shuffled_indice]\n",
    "\n",
    "    # percentage of train set.\n",
    "    train_frac = 0.7\n",
    "    train_index = int(0.7*S)\n",
    "    N_ls_train = N_ls_shuffled[0:train_index]\n",
    "    N_ls_valid = N_ls_shuffled[train_index:]\n",
    "    n_ls_train = n_ls_shuffled[0:train_index]\n",
    "    n_ls_valid = n_ls_shuffled[train_index:]\n",
    "\n",
    "    #reorder ground truth\n",
    "    ground_truth = ground_truth[shuffled_indice]\n",
    "    ground_truth_train = ground_truth[0:train_index]\n",
    "    ground_truth_valid = ground_truth[train_index:]\n",
    "    \n",
    "    return (N_ls_train, n_ls_train, ground_truth_train), (N_ls_valid, n_ls_valid, ground_truth_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some figures to get some visual impression of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot(N_ls_all, n_ls_all):\n",
    "    %matplotlib inline \n",
    "\n",
    "    fig, axes = plt.subplots(1,2,figsize=(14,6))\n",
    "    axes[0].scatter(N_ls_all-n_ls_all, n_ls_all)\n",
    "    axes[0].set_xlabel(\"N-n\",size=16)\n",
    "    axes[0].set_ylabel(\"n\",size=16)\n",
    "    axes[0].tick_params(labelsize=14)\n",
    "\n",
    "    axes[1].hist(n_ls_all/N_ls_all, bins=20)\n",
    "    axes[1].set_xlabel(\"n/N\", size=16)\n",
    "    axes[1].tick_params(labelsize=14)\n",
    "    axes[1].set_title(\"Histogram of n/N\", size=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the `log_likelihood` \n",
    "\n",
    "The `log_likelihood` is the log of the probability of the parameters, $\\Theta$, given the observed data, `N_ls` and `n_ls`. It is defined below,\n",
    "\n",
    "`log_likelihood` $= \\ln(L(\\Theta, {ni})) =\\ln( P({ni} | \\Theta)) = \\sum_{i=1}^{S} \\ln(\\sum_{k=1}^{K} \\pi_k * \\mathrm{Binom}(n_i|N_i, \\theta_k) )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate log_likelihood using a method which supposes to avoid underflow or overflow in\n",
    "# log_sum_exp = log(sum_{i=1}^{S} exp(bi)). When bi >> 1, overflow leads to log_sum_exp = infty\n",
    "# When bi << -1, underflow leads to log_sum_exp = -infty.\n",
    "\n",
    "def calc_logL(N_ls, n_ls, pi_list, theta_list):\n",
    "    '''\n",
    "    Input: N_ls is a [S] shape tensor = [N1, N2, ..., NS]\n",
    "           n_ls is a [S] shape tensor = [n1, n2, ..., nS]\n",
    "           pi_list is a [K] shape tensor = [pi_1, .., pi_K]\n",
    "           theta_list is a [K] shape tensor = [theta_1, ..., theta_K]\n",
    "    Output: log_likelihood of the parameters (pi and theta) given the observed data (N, n).\n",
    "    '''\n",
    "    S = len(N_ls)\n",
    "    K = len(pi_list)\n",
    "\n",
    "    # log_binom_mat has shape (S,K), element_{i,l} = log_Binomial(ni|Ni, theta_l)\n",
    "    # log with natural base.\n",
    "    log_binom_mat = Binomial(N_ls.reshape(S,1), theta_list.reshape(1,K)).log_prob(n_ls.reshape(S,1))\n",
    "\n",
    "    # mean_log_binom, the mean value of all elements in log_binom_mat.\n",
    "    c = torch.mean(log_binom_mat)\n",
    "\n",
    "    # binom_mat has shape (S,K), element_{i,l} = Binomial(ni|Ni, theta_l)\n",
    "    binom_mat = torch.exp(log_binom_mat - c)\n",
    "\n",
    "    # log_likelihood = sum_{i=1}^{S} log(prob_i), this is a real number\n",
    "    log_likelihood = S*c + torch.sum(torch.log(torch.matmul(binom_mat, pi_list)))\n",
    "\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $P(z_i=m| n_i, \\Theta_\\mathrm{old})$\n",
    "\n",
    "$P(z_i=m| n_i, \\Theta_\\mathrm{old}) = \\left[\\sum_{l=1}^{K} {\\pi_{l,old}\\over \\pi_{m,old}}\\left(\\theta_{l,old}\\over\\theta_{m,old}\\right)^{n_i}\\left({1-\\theta_{l,old}}\\over{1-\\theta_{m,old}}\\right)^{N_i-n_i}\\right]^{-1}$\n",
    "\n",
    "We take advantage of the [broadcast](https://pytorch.org/docs/stable/notes/broadcasting.html) nature for torch tensors. Any torch math manipulations, except element-wise manipulations, would take action in a broadcast way as long as the tensors are broadcastable. Broadcasting does not make copy of the data on the memory and thus is very efficient and much more preferred to `for loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Posterior(N_ls, n_ls, pi_list, theta_list):\n",
    "    '''\n",
    "    Input: N_ls is a [S] shape tensor = [N1, N2, ..., NS]\n",
    "           n_ls is a [S] shape tensor = [n1, n2, ..., nS]\n",
    "           pi_list is a [K] shape tensor = [pi_1, .., pi_K]\n",
    "           theta_list is a [K] shape tensor = [theta_1, ..., theta_K]\n",
    "    Output: Posterior, a tensor with shape (K,S) and its element_{m,i} = P(zi=m|ni,Theta_old) which is\n",
    "            the posterior probability of the i-th sample belonging to the m-th Binomial distribution.\n",
    "    '''\n",
    "    \n",
    "    # shape = (K,K) with theta_ratio_{m,l} = theta_l/theta_m, m-th row, l-th column\n",
    "    theta_ratio = torch.div(theta_list.reshape(1,K), theta_list.reshape(K,1))\n",
    "\n",
    "    # shape = (K,K), element_{ml} = (1-theta_l)/(1-theta_m)\n",
    "    unity_minus_theta_ratio = torch.div((1e0 - theta_list).reshape(1,K), (1e0 - theta_list).reshape(K,1))\n",
    "\n",
    "    # shape = (K,K), element_{m,l} = (theta_l/theta_m) * [(1-theta_l)/(1-theta_m)]\n",
    "    mixed_ratio = torch.mul(theta_ratio, unity_minus_theta_ratio)\n",
    "\n",
    "    # shape = (K,K,S) with element_{m,l,i} = [(theta_l/theta_m)*(1-theta_l)/(1-theta_m)]^ni\n",
    "    # its element won't be either 0 or infty no matther whether theta_l > or < theta_m\n",
    "    mixed_ratio_pow = torch.pow(theta_ratio.reshape(K,K,1), n_ls)\n",
    "    mixed_ratio_pow = torch.clamp(mixed_ratio_pow, min=0.0, max=1e15)\n",
    "\n",
    "    # shape = (K,K,S) with element_{m,l,i} = [ (1-theta_l)/(1-theta_m) ]^(Ni-2ni)\n",
    "    # its element may be infty if theta_l<<theta_m, or 0 if theta_l >> theta_m\n",
    "    unity_minus_theta_ratio_pow = torch.pow(unity_minus_theta_ratio.reshape(K,K,1), N_ls-2.0*n_ls)\n",
    "    unity_minus_theta_ratio_pow = torch.clamp(unity_minus_theta_ratio_pow, min=0.0, max=1e15)\n",
    "\n",
    "    # In below, we multiply the element of mixed_ratio_pow and the element of unity_minus_theta_ratio_pow,\n",
    "    # and there won't be nan caused by 0*infty or infty*0 because the element in mixed_ratio_pow won't be 0 or infty.\n",
    "    # Thus we make sure there won't be nan in Posterior.\n",
    "\n",
    "    # element-wise multiply, pow_tensor has shape(K,K,S), element_{m,l,i} = (theta_l/theta_m)^ni * [(1-theta_l)/(1-theta_m)]^(Ni-ni).\n",
    "    # Note that torch.mul(a, b) would broadcast if a and b are different in shape & they are\n",
    "    # broadcastable. If a and b are the same in shape, torch.mul(a,b) would operate element-wise multiplication.\n",
    "\n",
    "    pow_tensor = torch.mul(mixed_ratio_pow, unity_minus_theta_ratio_pow)\n",
    "\n",
    "    # pi_ratio has shape (K,K) with element_{m,l} = pi_l/pi_m\n",
    "    pi_ratio = torch.div(pi_list.reshape(1,K), pi_list.reshape(K,1))\n",
    "\n",
    "    # posterior probability tensor, Pzim = P(zi=m|ni,Theta_old)\n",
    "    # shape (K,S), element_{m,i} = P(zi=m|ni,Theta_old)\n",
    "    S = len(N_ls)\n",
    "    Posterior = torch.pow(torch.matmul(pi_ratio.reshape(K,1,K), pow_tensor), -1e0).reshape(K,S)\n",
    "\n",
    "    return Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating $\\Theta \\equiv \\{(\\pi_m, \\theta_m)|m=1,2,...,K\\}$ According to the EM Algorithm\n",
    "\n",
    "The computational complexity of the EM Algorithm is $S\\times K$ per iteration.\n",
    "\n",
    "$\\pi_m ={1\\over S} \\sum_{i=1}^{S} P(z_i=m| n_i, \\Theta_{old})$\n",
    "\n",
    "$\\theta_m = {{\\sum_{i=1}^{S} n_i P(z_i=m| n_i, \\Theta_{old})}\\over{\\sum_{j=1}^{S} N_j P(z_j=m| n_j, \\Theta_{old})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(N_ls, n_ls, Posterior):\n",
    "    '''\n",
    "    Input: N_ls, tensor of shape [S]\n",
    "           n_ls, tensor of shape [S]\n",
    "           Posterior, tensor of shape (K,S)\n",
    "    '''\n",
    "    # update pi_list\n",
    "    # torch.sum(tensor, n) sum over the n-th dimension of the tensor\n",
    "    # e.g. if tensor'shape is (K,S) and n=1, the resulting tensor has shape (K,)\n",
    "    # the m-th element is the sum_{i=1}^{S} tensor_{m,i}\n",
    "    pi_list = torch.sum(Posterior,1)/len(N_ls)\n",
    "\n",
    "    # update theta_list\n",
    "    theta_list = torch.div(torch.matmul(Posterior, n_ls), torch.matmul(Posterior, N_ls))\n",
    "\n",
    "    return pi_list, theta_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the parameters $\\Theta$\n",
    "\n",
    "We denote $\\Theta$ by `params` in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(K = 2, small_value = 1e-6):\n",
    "\n",
    "    # initialize pi's, make sure that the sum of all pi's is unity\n",
    "    # pi is drawn from a Uniform distribution bound by [small_value, 1)\n",
    "    from torch.distributions.uniform import Uniform\n",
    "    pi_list = Uniform(low=small_value, high=1e0).sample([K-1])\n",
    "    pi_K = t.FloatTensor([1e0]) - pi_list.sum()\n",
    "    pi_list = torch.cat([pi_list, pi_K], dim=0)\n",
    "\n",
    "    # initialize theta's, make sure that each theta satisfies 0<theta<1\n",
    "    from torch.distributions.normal import Normal\n",
    "    theta_list = torch.clamp(Normal(loc=0.5, scale=0.3).sample(t.IntTensor([K])), min=small_value, max=1e0-small_value)\n",
    "\n",
    "    # combine all pi and theta into a list of tuples called `params`, which is the capital Theta in my article\n",
    "    # params has the shape of K rows x 2 columns\n",
    "    params = torch.stack([pi_list, theta_list], dim=1)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(init_steps, K, N_ls, n_ls):\n",
    "    \n",
    "    best_log_likelihood = float('-inf')\n",
    "    \n",
    "    for i in np.arange(init_steps):\n",
    "\n",
    "        cond_step = True\n",
    "        cond_params = True\n",
    "        iter_step = 0\n",
    "        log_likelihood = float('-inf')\n",
    "\n",
    "        pi_list, theta_list = initialize_parameters()\n",
    "\n",
    "        while cond_step and cond_params:\n",
    "\n",
    "            # posterior probability tensor, Pzim = P(zi=m|ni,Theta_old)\n",
    "            # shape (K,S), element_{m,i} = P(zi=m|ni,Theta_old)\n",
    "            Posterior = calc_Posterior(N_ls, n_ls, pi_list, theta_list)\n",
    "\n",
    "            # calculate the new pi_list and theta_list\n",
    "            pi_list_new, theta_list_new = calc_params(N_ls, n_ls, Posterior)\n",
    "\n",
    "            # calculate the new log_likelihood\n",
    "            log_likelihood_new = calc_logL(N_ls, n_ls, pi_list_new, theta_list_new)\n",
    "\n",
    "            # calculate the change of the log-likelihood and theta\n",
    "            delta_log_likelihood = abs(log_likelihood_new - log_likelihood)\n",
    "            delta_theta_list = abs(theta_list_new - theta_list)\n",
    "\n",
    "            # update params \n",
    "            pi_list = pi_list_new\n",
    "            theta_list = theta_list_new\n",
    "\n",
    "            # update log_likelihood and theta\n",
    "            log_likelihood = log_likelihood_new\n",
    "            theta_list = theta_list_new\n",
    "\n",
    "            # increase iter_step by 1\n",
    "            iter_step += 1\n",
    "            \n",
    "            # update the conditions for the while loop\n",
    "            # cond_params = (delta_params > epsilon)\n",
    "            cond_params = (abs(delta_log_likelihood / log_likelihood) > tolerance_log_likelihood) or t.any(t.abs(delta_theta_list / theta_list) > tolerance_theta)\n",
    "            cond_step = t.BoolTensor([iter_step < max_step])\n",
    "\n",
    "            if log_likelihood_new > best_log_likelihood or best_log_likelihood == float('-inf'):\n",
    "                best_log_likelihood = log_likelihood_new\n",
    "                best_params = torch.stack([pi_list, theta_list], dim=1)\n",
    "                best_Posterior = Posterior\n",
    "\n",
    "    print(f\"Initialization = {i+1}, iterations = {iter_step}\")\n",
    "    print(f\"\\ndelta_log_likelihood = {delta_log_likelihood}\")\n",
    "    print(f\"log_likelihood ={log_likelihood}\")\n",
    "    params = torch.stack([pi_list, theta_list], dim=1)\n",
    "    print(f\"{params}\")\n",
    "\n",
    "    return best_Posterior, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_posteriors(Posterior):\n",
    "\n",
    "    # collapse probability densities\n",
    "    if Posterior.mean() > 0.5:\n",
    "        Posterior_assignments = Posterior < 0.5\n",
    "    else:\n",
    "        Posterior_assignments = Posterior > 0.5\n",
    "        \n",
    "    return Posterior_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(Posterior_assignments, ground_truth):\n",
    "    \n",
    "    P = (ground_truth == True).sum()\n",
    "    N = (ground_truth == False).sum()\n",
    "    Err_rate = P/(P+N)\n",
    "\n",
    "    print(\"Ground truth:\\nP={0}\\nN={1}\\nError rate={2}\\n\".format(P, N, Err_rate))\n",
    "\n",
    "    TP = ((Posterior_assignments == ground_truth) & (Posterior_assignments == True)).sum()\n",
    "    TN = ((Posterior_assignments == ground_truth) & (Posterior_assignments == False)).sum()\n",
    "    FP = ((Posterior_assignments != ground_truth) & (Posterior_assignments == True)).sum()\n",
    "    FN = ((Posterior_assignments != ground_truth) & (Posterior_assignments == False)).sum()\n",
    "\n",
    "    Precision = TP / (TP+FP)\n",
    "    Recall = TP / (TP+FN)\n",
    "\n",
    "    print(\"Results:\\nTP={0}\\nTN={1}\\nFP={2}\\nFN={3}\\nPrecision={4}\\nRecall={5}\".format(TP, TN, FP, FN, Precision, Recall))\n",
    "\n",
    "    Est_err_rate = (TP+FP)/(TP+TN+FP+FN)\n",
    "    print(\"Estimated error rate={0}\\nDiff={1}\".format(Est_err_rate, abs(Est_err_rate-Err_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Data by a Binomial Mixture Model\n",
    "\n",
    "The method would fit the parameters\n",
    "\n",
    "$\\Theta = \\{ (\\pi_k, \\theta_k) | k=1, 2, ..., K\\}$\n",
    "\n",
    "We need to pre-set K. Here we set $K=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Training===\n",
      "\n",
      "Initialization = 5, iterations = 2\n",
      "\n",
      "delta_log_likelihood = 24.0\n",
      "log_likelihood =-981272.0\n",
      "tensor([[9.9990e-01, 9.9913e-04],\n",
      "        [1.0214e-04, 6.9628e-01]])\n",
      "Ground truth:\n",
      "P=715\n",
      "N=6999285\n",
      "Error rate=0.0001021428543026559\n",
      "\n",
      "Results:\n",
      "TP=715\n",
      "TN=6999285\n",
      "FP=0\n",
      "FN=0\n",
      "Precision=1.0\n",
      "Recall=1.0\n",
      "Estimated error rate=0.0001021428543026559\n",
      "Diff=0.0\n",
      "\n",
      "===Validation===\n",
      "\n",
      "Ground truth:\n",
      "P=299\n",
      "N=2999701\n",
      "Error rate=9.966666402760893e-05\n",
      "\n",
      "Results:\n",
      "TP=299\n",
      "TN=2999701\n",
      "FP=0\n",
      "FN=0\n",
      "Precision=1.0\n",
      "Recall=1.0\n",
      "Estimated error rate=9.966666402760893e-05\n",
      "Diff=0.0\n",
      "used 14.157631874084473\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "init_steps = 5\n",
    "\n",
    "tolerance_log_likelihood = 0.001 # tolerance for the change of the log-likelihood\n",
    "tolerance_theta = 0.001 # tolerance for the change of theta\n",
    "max_step = int(1e1) # maximum steps for iteration\n",
    "\n",
    "K = 2\n",
    "S = int(1e7)\n",
    "k_len = 31\n",
    "theta_err = 0.70 # probability of a kmer being absent if the base is incorrect\n",
    "theta_cor = 0.001 # probability of a kmer being absent if the base is correct\n",
    "pi_err = 0.0001 # genome per base error rate (QV)\n",
    "training, validation = new_dataset(S, k_len, theta_err, theta_cor, pi_err)\n",
    "\n",
    "print(\"\\n===Training===\\n\")\n",
    "N_ls, n_ls, ground_truth = training\n",
    "Posterior, best_params = EM(init_steps, K, N_ls, n_ls)\n",
    "Posterior_assignments = assign_posteriors(Posterior[0])\n",
    "precision_recall(Posterior_assignments, ground_truth)\n",
    "\n",
    "print(\"\\n===Validation===\\n\")\n",
    "N_ls, n_ls, ground_truth = validation\n",
    "Posterior = calc_Posterior(N_ls, n_ls, *best_params)\n",
    "Posterior_assignments = assign_posteriors(Posterior[0])\n",
    "precision_recall(Posterior_assignments, ground_truth)\n",
    "\n",
    "#plt.hist(Posterior[0])\n",
    "#plt.show()\n",
    "\n",
    "print(f\"used {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
